{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from segformer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Attention\n",
    "\n",
    "In the next block, only the efficient attention is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientAttention(nn.Module):\n",
    "    \"\"\"\n",
    "        input  -> x:[B, D, H, W]\n",
    "        output ->   [B, D, H, W]\n",
    "    \n",
    "        in_channels:    int -> Embedding Dimension \n",
    "        key_channels:   int -> Key Embedding Dimension,   Best: (in_channels)\n",
    "        value_channels: int -> Value Embedding Dimension, Best: (in_channels or in_channels//2) \n",
    "        head_count:     int -> It divides the embedding dimension by the head_count and process each part individually\n",
    "        \n",
    "        Conv2D # of Params:  ((k_h * k_w * C_in) + 1) * C_out)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, key_channels, value_channels, head_count=1):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.key_channels = key_channels\n",
    "        self.head_count = head_count\n",
    "        self.value_channels = value_channels\n",
    "\n",
    "        self.keys = nn.Conv2d(in_channels, key_channels, 1) \n",
    "        self.queries = nn.Conv2d(in_channels, key_channels, 1)\n",
    "        self.values = nn.Conv2d(in_channels, value_channels, 1)\n",
    "        self.reprojection = nn.Conv2d(value_channels, in_channels, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        n, _, h, w = input_.size()\n",
    "        \n",
    "        keys = self.keys(input_).reshape((n, self.key_channels, h * w))\n",
    "        queries = self.queries(input_).reshape(n, self.key_channels, h * w)\n",
    "        values = self.values(input_).reshape((n, self.value_channels, h * w))\n",
    "        \n",
    "        head_key_channels = self.key_channels // self.head_count\n",
    "        head_value_channels = self.value_channels // self.head_count\n",
    "        \n",
    "        attended_values = []\n",
    "        for i in range(self.head_count):\n",
    "            key = F.softmax(keys[\n",
    "                :,\n",
    "                i * head_key_channels: (i + 1) * head_key_channels,\n",
    "                :\n",
    "            ], dim=2)\n",
    "            \n",
    "            query = F.softmax(queries[\n",
    "                :,\n",
    "                i * head_key_channels: (i + 1) * head_key_channels,\n",
    "                :\n",
    "            ], dim=1)\n",
    "                        \n",
    "            value = values[\n",
    "                :,\n",
    "                i * head_value_channels: (i + 1) * head_value_channels,\n",
    "                :\n",
    "            ]            \n",
    "            \n",
    "            context = key @ value.transpose(1, 2) # dk*dv\n",
    "            attended_value = (context.transpose(1, 2) @ query).reshape(n, head_value_channels, h, w) # n*dv            \n",
    "            attended_values.append(attended_value)\n",
    "                \n",
    "        aggregated_values = torch.cat(attended_values, dim=1)\n",
    "        attention = self.reprojection(aggregated_values)\n",
    "\n",
    "        return attention\n",
    "    \n",
    "    \n",
    "class EfficientTransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "        Input  -> x (Size: (b, (H*W), d)), H, W\n",
    "        Output -> (b, (H*W), d)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, key_dim, value_dim, head_count=1, token_mlp='mix'):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(in_dim)\n",
    "        self.attn = EfficientAttention(in_channels=in_dim, key_channels=key_dim,\n",
    "                                       value_channels=value_dim, head_count=1)\n",
    "        self.norm2 = nn.LayerNorm(in_dim)\n",
    "        if token_mlp=='mix':\n",
    "            self.mlp = MixFFN(in_dim, int(in_dim*4))  \n",
    "        elif token_mlp=='mix_skip':\n",
    "            self.mlp = MixFFN_skip(in_dim, int(in_dim*4)) \n",
    "        else:\n",
    "            self.mlp = MLP_FFN(in_dim, int(in_dim*4))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, H, W) -> torch.Tensor:\n",
    "        norm_1 = self.norm1(x)\n",
    "        norm_1 = Rearrange('b (h w) d -> b d h w', h=H, w=W)(norm_1)\n",
    "        \n",
    "        attn = self.attn(norm_1)\n",
    "        attn = Rearrange('b d h w -> b (h w) d')(attn)\n",
    "        \n",
    "        tx = x + attn\n",
    "        mx = tx + self.mlp(self.norm2(tx), H, W)\n",
    "        return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 196, 320])\n"
     ]
    }
   ],
   "source": [
    "# test the efficient attention block with the same input as the first layer of the Transformer UNet\n",
    "x = torch.rand((6, 196, 320))\n",
    "efficient_block = EfficientTransformerBlock(320, 320, 320)\n",
    "test = efficient_block.forward(x, 14, 14)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient + Channel Attention\n",
    "\n",
    "This block shows how the efficient and channel attention is combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    \"\"\"\n",
    "        Input -> x: [B, N, C]\n",
    "        Output -> [B, N, C]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0, proj_drop=0):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" x: [B, N, C]\n",
    "        \"\"\"\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q.transpose(-2, -1)\n",
    "        k = k.transpose(-2, -1)\n",
    "        v = v.transpose(-2, -1)\n",
    "\n",
    "        q = F.normalize(q, dim=-1)\n",
    "        k = F.normalize(k, dim=-1)\n",
    "    \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.temperature\n",
    "        # -------------------\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).permute(0, 3, 1, 2).reshape(B, N, C)\n",
    "        # ------------------\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class DualTransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "        Input  -> x (Size: (b, (H*W), d)), H, W\n",
    "        Output -> (b, (H*W), d)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, key_dim, value_dim, head_count=1, token_mlp='mix'):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(in_dim)\n",
    "        self.attn = EfficientAttention(in_channels=in_dim, key_channels=key_dim,\n",
    "                                       value_channels=value_dim, head_count=1)\n",
    "        self.norm2 = nn.LayerNorm(in_dim)\n",
    "        self.norm3 = nn.LayerNorm(in_dim)\n",
    "        self.channel_attn = ChannelAttention(in_dim)\n",
    "        self.norm4 = nn.LayerNorm(in_dim)\n",
    "        # add channel attention here\n",
    "        if token_mlp=='mix':\n",
    "            self.mlp1 = MixFFN(in_dim, int(in_dim*4))\n",
    "            self.mlp2 = MixFFN(in_dim, int(in_dim*4))\n",
    "        elif token_mlp=='mix_skip':\n",
    "            self.mlp1 = MixFFN_skip(in_dim, int(in_dim*4))\n",
    "            self.mlp2 = MixFFN_skip(in_dim, int(in_dim*4))\n",
    "        else:\n",
    "            self.mlp1 = MLP_FFN(in_dim, int(in_dim*4))\n",
    "            self.mlp2 = MLP_FFN(in_dim, int(in_dim*4))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, H, W) -> torch.Tensor:\n",
    "        # dual attention structure like it is used in the davit\n",
    "        norm1 = self.norm1(x)\n",
    "        norm1 = Rearrange('b (h w) d -> b d h w', h=H, w=W)(norm1)\n",
    "        \n",
    "        attn = self.attn(norm1)\n",
    "        attn = Rearrange('b d h w -> b (h w) d')(attn)\n",
    "\n",
    "        add1 = x + attn\n",
    "        norm2 = self.norm2(add1)\n",
    "        mlp1 = self.mlp1(norm2, H, W)\n",
    "\n",
    "        add2 = add1 + mlp1\n",
    "        norm3 = self.norm3(add2)\n",
    "        channel_attn = self.channel_attn(norm3)\n",
    "\n",
    "        add3 = add2 + channel_attn\n",
    "        norm4 = self.norm4(add3)\n",
    "        mlp2 = self.mlp2(norm4, H, W)\n",
    "        \n",
    "        mx = add3 + mlp2\n",
    "        return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 196, 320])\n"
     ]
    }
   ],
   "source": [
    "# same input as before, but with the Dual Attention\n",
    "x = torch.rand((6, 196, 320))\n",
    "dual_block = DualTransformerBlock(320, 320, 320)\n",
    "test = dual_block.forward(x, 14, 14)\n",
    "print(test.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('new_cuda')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa78b0d7c19612feb57a86eb987f628f19562ccd1d4123a203552ea860c47f43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
